{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\tomas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\tomas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from keras_ocr.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F2R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for C:\\Users\\tomas\\.keras-ocr\\craft_mlt_25k.h5\n",
      "WARNING:tensorflow:From c:\\Users\\tomas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\tomas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260: resize_bilinear (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.image.resize(...method=ResizeMethod.BILINEAR...)` instead.\n",
      "WARNING:tensorflow:From c:\\Users\\tomas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Looking for C:\\Users\\tomas\\.keras-ocr\\crnn_kurapan.h5\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "25 sec of video left.\n",
      "22 sec of video left.\n",
      "19 sec of video left.\n",
      "16 sec of video left.\n",
      "13 sec of video left.\n",
      "10 sec of video left.\n",
      "7 sec of video left.\n",
      "4 sec of video left.\n",
      "1 sec of video left.\n",
      "End of input video. Exiting...\n"
     ]
    }
   ],
   "source": [
    "OUT_VIDEO_NAME = 'output_F2R.avi'\n",
    "\n",
    "BLACK = 0\n",
    "WHITE = 255\n",
    "GAUSSIAN_BLUR = (5,5)\n",
    "THRESHOLD_GOOD_MATCH = 0.6\n",
    "BG_INTENSITY_MIN = 50\n",
    "BG_INTENSITY_MAX = 125\n",
    "THRESHOLD_LOGO_MASK = 200\n",
    "\n",
    "def inits():\n",
    "    cap = cv2.VideoCapture('imgs/Multiple View.avi')\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    out = cv2.VideoWriter('output_F2R.avi', cv2.VideoWriter_fourcc(*'DIVX'), fps, (w,h))\n",
    "    sift = cv2.SIFT_create()\n",
    "    flann = cv2.FlannBasedMatcher(dict(algorithm=1, trees=5), dict(checks=50))\n",
    "    return cap, out, w, h, sift, flann\n",
    "\n",
    "def textBackgroundMatching(augmented_layer):\n",
    "    text = augmented_layer.copy()\n",
    "    text[:text.shape[0]//2,:] = BLACK\n",
    "    text_gray = cv2.cvtColor(text, cv2.COLOR_RGB2GRAY)\n",
    "    mask = (text_gray > BG_INTENSITY_MIN) & (text_gray < BG_INTENSITY_MAX)\n",
    "    warped_al[mask] = curr_frame[mask]\n",
    "    return warped_al\n",
    "\n",
    "def splitAlMask(mask):\n",
    "    logo_mask = mask.copy()\n",
    "    logo_mask[h//2:,:] = BLACK\n",
    "    text_mask = mask.copy()\n",
    "    text_mask[:h//2,:] = BLACK\n",
    "    return logo_mask, text_mask\n",
    "\n",
    "def findTextCorners(img):\n",
    "    prediction_groups = Pipeline().recognize([img])\n",
    "    corners_list = [[] for _ in range(8)]\n",
    "    for box in prediction_groups[0]:\n",
    "        if box[0] in ['richard', 'hartley', 'and', 'andrew', 'zisserman']:\n",
    "            for i in range(4):\n",
    "                corners_list[2*i].append(box[1][i][0])\n",
    "                corners_list[2*i+1].append(box[1][i][1])\n",
    "\n",
    "    full_corners = np.array([[min(corners_list[0]), min(corners_list[1])],\n",
    "                             [max(corners_list[2]), min(corners_list[3])],\n",
    "                             [max(corners_list[4]), max(corners_list[5])],\n",
    "                             [min(corners_list[6]), max(corners_list[7])]],dtype='float32')\n",
    "\n",
    "    return full_corners.reshape(1, -1, 2)\n",
    "\n",
    "def removeAuthorText(img, M, text_corners): \n",
    "    corners = cv2.perspectiveTransform(text_corners, M)[0]\n",
    "\n",
    "    x_mid0, y_mid0 = int((corners[1][0] + corners[2][0])/2), int((corners[1][1] + corners[2][1])/2)\n",
    "    x_mid1, y_mi1 = int((corners[0][0] + corners[3][0])/2), int((corners[0][1] + corners[3][1])/2)\n",
    "    \n",
    "    mask = np.zeros(img.shape[:2], dtype=\"uint8\")\n",
    "    thickness = int(sqrt( (corners[2][0] - corners[1][0])**2 + (corners[2][1] - corners[1][1])**2 ))\n",
    "    cv2.line(mask, (x_mid0, y_mid0), (x_mid1, y_mi1), 255, thickness+1)\n",
    "    \n",
    "    return cv2.inpaint(img, mask, 7, cv2.INPAINT_NS)\n",
    "\n",
    "\n",
    "# Initializations\n",
    "cap, out, w, h, sift, flann = inits()\n",
    "\n",
    "# Read first frame, augmented layer and masks\n",
    "_, ref_frame = cap.read()\n",
    "ref_frame = cv2.cvtColor(ref_frame, cv2.COLOR_BGR2RGB)\n",
    "object_mask = cv2.imread('imgs/ObjectMask.PNG', cv2.IMREAD_GRAYSCALE)\n",
    "al = cv2.imread('imgs/AugmentedLayer.PNG')[:ref_frame.shape[0], :ref_frame.shape[1]]\n",
    "al = cv2.cvtColor(al, cv2.COLOR_BGR2RGB)\n",
    "al_mask = cv2.imread('imgs/AugmentedLayerMask.PNG', cv2.IMREAD_GRAYSCALE)[:ref_frame.shape[0], :ref_frame.shape[1]]\n",
    "\n",
    "# Blur augmented layer's mask for smoother edges on result\n",
    "al_mask = cv2.GaussianBlur(al_mask, GAUSSIAN_BLUR, 0)\n",
    "\n",
    "# Mask reference frame\n",
    "ref_frame[object_mask == BLACK] = BLACK\n",
    "\n",
    "# Find keypoints and compute descriptions in reference frame\n",
    "kp_ref, des_ref = sift.detectAndCompute(ref_frame, None)\n",
    "\n",
    "# Find corners of text present in both frame and augmented layer and remove it using keras OCR\n",
    "org_text_corners = findTextCorners(ref_frame)\n",
    "\n",
    "frame_num = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, curr_frame = cap.read()\n",
    "    if not ret or curr_frame is None:\n",
    "        print('End of input video. Exiting...')\n",
    "        break\n",
    "    curr_frame = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Find keypoints and descriptions in current frame and match with reference frame\n",
    "    kp_frame, des_frame = sift.detectAndCompute(curr_frame, None)\n",
    "    matches = flann.knnMatch(des_ref, des_frame, k=2)\n",
    "\n",
    "    good = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < THRESHOLD_GOOD_MATCH*n.distance:\n",
    "            good.append(m)\n",
    "\n",
    "    if len(good) < 4:\n",
    "        print(\"Not enough good matches. Aborting...\")\n",
    "        break\n",
    "    \n",
    "    # build corrspondence arrays of good matches\n",
    "    src_pts = np.float32([kp_ref[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    dst_pts = np.float32([kp_frame[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "\n",
    "    # Estimate a robust homography with RANSAC and warp augmented layer and its mask\n",
    "    M, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 2.0)\n",
    "    warped_al = cv2.warpPerspective(al, M, (w,h))\n",
    "    warped_al_mask = cv2.warpPerspective(al_mask, M, (w,h), flags=cv2.INTER_NEAREST)\n",
    "\n",
    "    # Remove text that is both in frame and in augmented layer\n",
    "    curr_frame = removeAuthorText(curr_frame, M, org_text_corners)\n",
    "    \n",
    "    # Match text's background in the augmented layer with the one on the book \n",
    "    warped_al = textBackgroundMatching(warped_al)\n",
    "\n",
    "    # Split augmented layer's mask into its two parts\n",
    "    warped_al_logo_mask, warped_al_text_mask = splitAlMask(warped_al_mask)\n",
    "\n",
    "    # Place both parts of augmented layer on current frame and write to video\n",
    "    curr_frame[warped_al_logo_mask>THRESHOLD_LOGO_MASK] = warped_al[warped_al_logo_mask>THRESHOLD_LOGO_MASK]\n",
    "    curr_frame[warped_al_text_mask==WHITE] = warped_al[warped_al_text_mask==WHITE]\n",
    "\n",
    "    out.write(cv2.cvtColor(curr_frame, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "    frame_num += 1\n",
    "    if frame_num % 45 == 0:\n",
    "        print(round(28-frame_num/15), \"sec of video left.\")\n",
    "\n",
    "cap.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F2F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for C:\\Users\\tomas\\.keras-ocr\\craft_mlt_25k.h5\n",
      "Looking for C:\\Users\\tomas\\.keras-ocr\\crnn_kurapan.h5\n",
      "1/1 [==============================] - 9s 9s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "25 sec of video left.\n",
      "22 sec of video left.\n",
      "19 sec of video left.\n",
      "16 sec of video left.\n",
      "13 sec of video left.\n",
      "10 sec of video left.\n",
      "7 sec of video left.\n",
      "4 sec of video left.\n",
      "1 sec of video left.\n",
      "End of input video. Exiting...\n"
     ]
    }
   ],
   "source": [
    "OUT_VIDEO_NAME = 'output_F2F.avi'\n",
    "\n",
    "BLACK = 0\n",
    "WHITE = 255\n",
    "GAUSSIAN_BLUR = (5,5)\n",
    "THRESHOLD_GOOD_MATCH = 0.6\n",
    "BG_INTENSITY_MIN = 50\n",
    "BG_INTENSITY_MAX = 125\n",
    "THRESHOLD_LOGO_MASK = 200\n",
    "\n",
    "\n",
    "def inits():\n",
    "    cap = cv2.VideoCapture('imgs/Multiple View.avi')\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    out = cv2.VideoWriter(OUT_VIDEO_NAME, cv2.VideoWriter_fourcc(*'DIVX'), fps, (w,h))\n",
    "    sift = cv2.SIFT_create()\n",
    "    flann = cv2.FlannBasedMatcher(dict(algorithm=1, trees=5), dict(checks=50))\n",
    "    return cap, out, w, h, sift, flann\n",
    "\n",
    "def textBackgroundMatching(augmented_layer):\n",
    "    text = augmented_layer.copy()\n",
    "    text[:text.shape[0]//2,:] = BLACK\n",
    "    text_gray = cv2.cvtColor(text, cv2.COLOR_RGB2GRAY)\n",
    "    mask = (text_gray > BG_INTENSITY_MIN) & (text_gray < BG_INTENSITY_MAX)\n",
    "    warped_al[mask] = curr_frame[mask]\n",
    "    return warped_al\n",
    "\n",
    "def splitAlMask(mask):\n",
    "    logo_mask = mask.copy()\n",
    "    logo_mask[h//2:,:] = BLACK\n",
    "    text_mask = mask.copy()\n",
    "    text_mask[:h//2,:] = BLACK\n",
    "    return logo_mask, text_mask\n",
    "\n",
    "def findTextCorners(img):\n",
    "    prediction_groups = Pipeline().recognize([img])\n",
    "    corners_list = [[] for _ in range(8)]\n",
    "    for box in prediction_groups[0]:\n",
    "        if box[0] in ['richard', 'hartley', 'and', 'andrew', 'zisserman']:\n",
    "            for i in range(4):\n",
    "                corners_list[2*i].append(box[1][i][0])\n",
    "                corners_list[2*i+1].append(box[1][i][1])\n",
    "\n",
    "    full_corners = np.array([[min(corners_list[0]), min(corners_list[1])],\n",
    "                             [max(corners_list[2]), min(corners_list[3])],\n",
    "                             [max(corners_list[4]), max(corners_list[5])+3],\n",
    "                             [min(corners_list[6]), max(corners_list[7])+4]],dtype='float32')\n",
    "\n",
    "    return full_corners.reshape(1, -1, 2)\n",
    "\n",
    "def removeAuthorText(img, M, text_corners): \n",
    "    corners = cv2.perspectiveTransform(text_corners, M)[0]\n",
    "\n",
    "    x_mid0, y_mid0 = int((corners[1][0] + corners[2][0])/2), int((corners[1][1] + corners[2][1])/2)\n",
    "    x_mid1, y_mi1 = int((corners[0][0] + corners[3][0])/2), int((corners[0][1] + corners[3][1])/2)\n",
    "    \n",
    "    mask = np.zeros(img.shape[:2], dtype=\"uint8\")\n",
    "    thickness = int(sqrt( (corners[2][0] - corners[1][0])**2 + (corners[2][1] - corners[1][1])**2 ))\n",
    "    cv2.line(mask, (x_mid0, y_mid0), (x_mid1, y_mi1), 255, thickness+1)\n",
    "    \n",
    "    return cv2.inpaint(img, mask, 7, cv2.INPAINT_NS)\n",
    "\n",
    "\n",
    "# Initializations\n",
    "cap, out, w, h, sift, flann = inits()\n",
    "\n",
    "# Read first frame, augmented layer and masks\n",
    "_, prev_frame = cap.read()\n",
    "prev_frame = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2RGB)\n",
    "object_mask = cv2.imread('imgs/ObjectMask.PNG', cv2.IMREAD_GRAYSCALE)\n",
    "al = cv2.imread('imgs/AugmentedLayer.PNG')[:prev_frame.shape[0], :prev_frame.shape[1]]\n",
    "al = cv2.cvtColor(al, cv2.COLOR_BGR2RGB)\n",
    "al_mask = cv2.imread('imgs/AugmentedLayerMask.PNG', cv2.IMREAD_GRAYSCALE)[:prev_frame.shape[0], :prev_frame.shape[1]]\n",
    "\n",
    "# Blur augmented layer's mask for smoother edges on result\n",
    "al_mask = cv2.GaussianBlur(al_mask, GAUSSIAN_BLUR, 0)\n",
    "\n",
    "# Mask reference frame\n",
    "prev_frame[object_mask == BLACK] = BLACK\n",
    "\n",
    "# Find keypoints and compute descriptions in reference frame\n",
    "kp_prev, des_prev = sift.detectAndCompute(prev_frame, None)\n",
    "\n",
    "# Find corners of text present in both frame and augmented layer and remove it using keras OCR\n",
    "org_text_corners = findTextCorners(prev_frame)\n",
    "\n",
    "# Define the first homography as the identity matrix\n",
    "M = np.eye(3)\n",
    "\n",
    "frame_num = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, curr_frame = cap.read()\n",
    "    if not ret or curr_frame is None:\n",
    "        print('End of input video. Exiting...')\n",
    "        break\n",
    "    curr_frame = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Find keypoints and compute descriptions in current frame\n",
    "    kp_curr, des_curr = sift.detectAndCompute(curr_frame, None)\n",
    "\n",
    "    matches = flann.knnMatch(des_prev, des_curr, k=2)\n",
    "\n",
    "    good = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < THRESHOLD_GOOD_MATCH*n.distance:\n",
    "            good.append(m)\n",
    "\n",
    "    if len(good) < 4:\n",
    "        print(\"Not enough good matches. Aborting...\")\n",
    "        break\n",
    "    \n",
    "    # building the corrspondences arrays of good matches\n",
    "    src_pts = np.float32([kp_prev[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    dst_pts = np.float32([kp_curr[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "\n",
    "    # Estimate a robust homography with RANSAC and multiply it with previous homography\n",
    "    M_new, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "    M = M_new @ M\n",
    "\n",
    "    # Warp augmented layer and masks\n",
    "    warped_al = cv2.warpPerspective(al, M, (w,h))\n",
    "    warped_al_mask = cv2.warpPerspective(al_mask, M, (w,h), flags=cv2.INTER_NEAREST)\n",
    "    warped_object_mask = cv2.warpPerspective(object_mask, M, (w,h))\n",
    "\n",
    "    # Remove text that is both in frame and in augmented layer\n",
    "    curr_frame = removeAuthorText(curr_frame, M, org_text_corners)\n",
    "\n",
    "    # Split augmented layer's mask into its two parts\n",
    "    warped_al_logo_mask, warped_al_text_mask = splitAlMask(warped_al_mask)\n",
    "\n",
    "    # Match text's background in the augmented layer with the one on the book \n",
    "    warped_al = textBackgroundMatching(warped_al)\n",
    "\n",
    "    prev_frame = np.copy(curr_frame)\n",
    "    prev_frame[warped_object_mask==BLACK] = BLACK\n",
    "\n",
    "    kp_prev = tuple(kp_curr)\n",
    "    des_prev = np.copy(des_curr)\n",
    "\n",
    "    # Place both parts of augmented layer on current frame and write to video\n",
    "    curr_frame[warped_al_logo_mask>THRESHOLD_LOGO_MASK] = warped_al[warped_al_logo_mask>THRESHOLD_LOGO_MASK]\n",
    "    curr_frame[warped_al_text_mask==WHITE] = warped_al[warped_al_text_mask==WHITE]\n",
    "    \n",
    "    out.write(cv2.cvtColor(curr_frame, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "    frame_num += 1\n",
    "    if frame_num % 45 == 0:\n",
    "        print(round(28-frame_num/15), \"sec of video left.\")\n",
    "\n",
    "cap.release()\n",
    "out.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
