{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import keras_ocr\n",
    "import numpy as np\n",
    "from sys import exit\n",
    "from math import sqrt\n",
    "from  os import environ\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Suppress warnings (from Tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test F2F with removing background text with Keras\n",
    "\n",
    "Source: [Remove Text from Images using CV2 and Keras-OCR](https://towardsdatascience.com/remove-text-from-images-using-cv2-and-keras-ocr-24e7612ae4f4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_VIDEO_NAME = 'output_F2F_ML.avi'\n",
    "\n",
    "REMOVE_FIRST_TEXT_AL = False\n",
    "\n",
    "BLACK = 0\n",
    "WHITE = 255\n",
    "GAUSSIAN_BLUR = (5,5)\n",
    "THRESHOLD_GOOD_MATCH = 0.6\n",
    "BG_INTENSITY_MIN = 50\n",
    "BG_INTENSITY_MAX = 125\n",
    "THRESHOLD_LOGO_MASK = 200\n",
    "\n",
    "\n",
    "def inits():\n",
    "    cap = cv2.VideoCapture('imgs/Multiple View.avi')\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    out = cv2.VideoWriter(OUT_VIDEO_NAME, cv2.VideoWriter_fourcc(*'DIVX'), fps, (w,h))\n",
    "    sift = cv2.SIFT_create()\n",
    "    flann = cv2.FlannBasedMatcher(dict(algorithm=1, trees=5), dict(checks=50))\n",
    "    return cap, out, w, h, sift, flann\n",
    "\n",
    "def textBackgroundMatching(augmented_layer):\n",
    "    text = augmented_layer.copy()\n",
    "    text[:text.shape[0]//2,:] = BLACK\n",
    "    text_gray = cv2.cvtColor(text, cv2.COLOR_RGB2GRAY)\n",
    "    mask = (text_gray > BG_INTENSITY_MIN) & (text_gray < BG_INTENSITY_MAX)\n",
    "    warped_al[mask] = curr_frame[mask]\n",
    "    return warped_al\n",
    "\n",
    "def splitAlMask(mask):\n",
    "    logo_mask = mask.copy()\n",
    "    logo_mask[h//2:,:] = BLACK\n",
    "    text_mask = mask.copy()\n",
    "    text_mask[:h//2,:] = BLACK\n",
    "    return logo_mask, text_mask\n",
    "\n",
    "def removeAuthorText(img):\n",
    "    # generate (word, box) tuples\n",
    "    prediction_groups = keras_ocr.pipeline.Pipeline().recognize([img])\n",
    "    mask = np.zeros(img.shape[:2], dtype=\"uint8\")\n",
    "\n",
    "    for box in prediction_groups[0]:\n",
    "        if box[0] in ['richard', 'hartley', 'and', 'andrew', 'zisserman']:\n",
    "            x0, y0 = box[1][0]\n",
    "            x1, y1 = box[1][1] \n",
    "            x2, y2 = box[1][2]\n",
    "            x3, y3 = box[1][3] \n",
    "\n",
    "            x_mid0, y_mid0 = int((x1 + x2)/2), int((y1 + y2)/2)\n",
    "            x_mid1, y_mi1 = int((x0 + x3)/2), int((y0 + y3)/2)\n",
    "        \n",
    "            thickness = int(sqrt( (x2 - x1)**2 + (y2 - y1)**2 ))\n",
    "        \n",
    "            cv2.line(mask, (x_mid0, y_mid0), (x_mid1, y_mi1), 255, thickness)\n",
    "            img = cv2.inpaint(img, mask, 7, cv2.INPAINT_NS)\n",
    "                 \n",
    "    return img\n",
    "\n",
    "\n",
    "# Initializations\n",
    "cap, out, w, h, sift, flann = inits()\n",
    "\n",
    "# Read first frame, augmented layer and masks\n",
    "_, prev_frame = cap.read()\n",
    "prev_frame = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "object_mask = cv2.imread('imgs/ObjectMask.PNG', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "al = cv2.imread('imgs/AugmentedLayer.PNG')[:prev_frame.shape[0], :prev_frame.shape[1]]\n",
    "al = cv2.cvtColor(al, cv2.COLOR_BGR2RGB)\n",
    "if REMOVE_FIRST_TEXT_AL: al[300:361, :] = BLACK\n",
    "\n",
    "al_mask = cv2.imread('imgs/AugmentedLayerMask.PNG', cv2.IMREAD_GRAYSCALE)[:prev_frame.shape[0], :prev_frame.shape[1]]\n",
    "if REMOVE_FIRST_TEXT_AL: al_mask[300:361, :] = BLACK\n",
    "\n",
    "# Blur augmented layer's mask for smoother edges on result\n",
    "al_mask = cv2.GaussianBlur(al_mask, GAUSSIAN_BLUR, 0)\n",
    "\n",
    "# Mask reference frame\n",
    "prev_frame[object_mask == BLACK] = BLACK\n",
    "\n",
    "# Find keypoints and compute descriptions in reference frame\n",
    "kp_prev, des_prev = sift.detectAndCompute(prev_frame, None)\n",
    "\n",
    "# Define the first homography as the identity matrix\n",
    "M = np.array([[1,0,0], [0,1,0], [0,0,1]])\n",
    "\n",
    "frame_num = 0\n",
    "while cap.isOpened():\n",
    "    ret, curr_frame = cap.read()\n",
    "    if not ret or curr_frame is None:\n",
    "        print('End of input video. Exiting...')\n",
    "        break\n",
    "    curr_frame = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Find keypoints and compute descriptions in current frame\n",
    "    kp_curr, des_curr = sift.detectAndCompute(curr_frame, None)\n",
    "\n",
    "    matches = flann.knnMatch(des_prev, des_curr, k=2)\n",
    "\n",
    "    good = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < THRESHOLD_GOOD_MATCH*n.distance:\n",
    "            good.append(m)\n",
    "\n",
    "    if len(good) < 4:\n",
    "        print(\"Not enough good matches. Aborting...\")\n",
    "        break\n",
    "    \n",
    "    # building the corrspondences arrays of good matches\n",
    "    src_pts = np.float32([kp_prev[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    dst_pts = np.float32([kp_curr[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "\n",
    "    # Estimate a robust homography with RANSAC and multiply it with previous homography\n",
    "    M_new, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "    M = M_new @ M\n",
    "\n",
    "    # Warp augmented layer and masks\n",
    "    warped_al = cv2.warpPerspective(al, M, (w,h))\n",
    "    warped_al_mask = cv2.warpPerspective(al_mask, M, (w,h), flags=cv2.INTER_NEAREST)\n",
    "    warped_object_mask = cv2.warpPerspective(object_mask, M, (w,h))\n",
    "\n",
    "    # Split augmented layer's mask into its two parts\n",
    "    warped_al_logo_mask, warped_al_text_mask = splitAlMask(warped_al_mask)\n",
    "\n",
    "    curr_frame = removeAuthorText(curr_frame)\n",
    "\n",
    "    # Match text's background in the augmented layer with the one on the book \n",
    "    warped_al = textBackgroundMatching(warped_al)\n",
    "\n",
    "    prev_frame = np.copy(curr_frame)\n",
    "    prev_frame[warped_object_mask==BLACK] = BLACK\n",
    "\n",
    "    kp_prev = tuple(kp_curr)\n",
    "    des_prev = np.copy(des_curr)\n",
    "\n",
    "    # Place both parts of augmented layer on current frame and write to video\n",
    "    curr_frame[warped_al_logo_mask>THRESHOLD_LOGO_MASK] = warped_al[warped_al_logo_mask>THRESHOLD_LOGO_MASK]\n",
    "    curr_frame[warped_al_text_mask==WHITE] = warped_al[warped_al_text_mask==WHITE]\n",
    "    \n",
    "    out.write(cv2.cvtColor(curr_frame, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "    frame_num += 1\n",
    "    if frame_num % 15 == 0:\n",
    "        print(round((27*15-frame_num)/15), \"sec of video left.\")\n",
    "    \n",
    "\n",
    "cap.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeAuthorText(img):\n",
    "    # generate (word, box) tuples \n",
    "    prediction_groups = keras_ocr.pipeline.Pipeline().recognize([img])\n",
    "    mask = np.zeros(img.shape[:2], dtype=\"uint8\")\n",
    "    for box in prediction_groups[0]:\n",
    "        if box[0] in ['richard', 'hartley', 'and', 'andrew', 'zisserman']:\n",
    "            x0, y0 = box[1][0]\n",
    "            x1, y1 = box[1][1] \n",
    "            x2, y2 = box[1][2]\n",
    "            x3, y3 = box[1][3] \n",
    "\n",
    "            x_mid0, y_mid0 = int((x1 + x2)/2), int((y1 + y2)/2)\n",
    "            x_mid1, y_mi1 = int((x0 + x3)/2), int((y0 + y3)/2)\n",
    "        \n",
    "            thickness = int(sqrt( (x2 - x1)**2 + (y2 - y1)**2 ))\n",
    "        \n",
    "            cv2.line(mask, (x_mid0, y_mid0), (x_mid1, y_mi1), 255, thickness)\n",
    "            img = cv2.inpaint(img, mask, 7, cv2.INPAINT_NS)\n",
    "                 \n",
    "    return(img)\n",
    "\n",
    "img_in = cv2.imread('imgs/ReferenceFrame.png')\n",
    "img_in = cv2.cvtColor(img_in, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "out = removeAuthorText(img_in)\n",
    "\n",
    "plt.imshow(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeAuthorText(img):\n",
    "    # generate (word, box) tuples \n",
    "    prediction_groups = keras_ocr.pipeline.Pipeline().recognize([img])\n",
    "    mask = np.zeros(img.shape[:2], dtype=\"uint8\")\n",
    "    for box in prediction_groups[0]:\n",
    "        if box[0] in ['richard', 'hartley', 'and', 'andrew', 'zisserman']:\n",
    "            x0, y0 = box[1][0]\n",
    "            x1, y1 = box[1][1] \n",
    "            x2, y2 = box[1][2]\n",
    "            x3, y3 = box[1][3] \n",
    "            \n",
    "            print(x0, y0, ' \\n', x1, y1, ' \\n', x2, y2, ' \\n', x3, y3, ' \\n',)\n",
    "            x_mid0, y_mid0 = int((x1 + x2)/2), int((y1 + y2)/2)\n",
    "            x_mid1, y_mi1 = int((x0 + x3)/2), int((y0 + y3)/2)\n",
    "        \n",
    "            thickness = int(sqrt( (x2 - x1)**2 + (y2 - y1)**2 ))\n",
    "        \n",
    "            cv2.line(mask, (x_mid0, y_mid0), (x_mid1, y_mi1), 255, thickness)\n",
    "            img = cv2.inpaint(img, mask, 7, cv2.INPAINT_NS)\n",
    "                 \n",
    "    return(img)\n",
    "\n",
    "img_in = cv2.imread('imgs/ReferenceFrame.png')\n",
    "img_in = cv2.cvtColor(img_in, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "out = removeAuthorText(img_in)\n",
    "\n",
    "plt.imshow(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('imgs/ReferenceFrame.png')\n",
    "img = cv2.cvtColor(img_in, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "def removeAuthorText(img, M, corners_org):\n",
    "    \n",
    "    \n",
    "    corners = cv2.perspectiveTransform(corners_org, M)[0]\n",
    "\n",
    "    x_mid0, y_mid0 = int((corners[1][0] + corners[2][0])/2), int((corners[1][1] + corners[2][1])/2)\n",
    "    x_mid1, y_mi1 = int((corners[0][0] + corners[3][0])/2), int((corners[0][1] + corners[3][1])/2)\n",
    "\n",
    "    thickness = int(sqrt( (corners[2][0] - corners[1][0])**2 + (corners[2][1] - corners[1][1])**2 ))\n",
    "    \n",
    "    mask = np.zeros(img.shape[:2], dtype=\"uint8\")\n",
    "\n",
    "    cv2.line(mask, (x_mid0, y_mid0), (x_mid1, y_mi1), 255, thickness+1)\n",
    "    img = cv2.inpaint(img, mask, 7, cv2.INPAINT_NS)\n",
    "    \n",
    "    return img\n",
    "\n",
    "M = np.eye(3)\n",
    "\n",
    "text_corners = np.array([[232.0, 345.0],\n",
    "                         [455.0, 345.0],\n",
    "                         [455.0, 361.0],\n",
    "                         [232.0, 361.0]], dtype='float32').reshape(1, -1, 2)\n",
    "\n",
    "imgout = removeAuthorText(img, M, text_corners)\n",
    "\n",
    "plt.imshow(imgout)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
